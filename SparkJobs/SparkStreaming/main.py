import os
import sys
import string
import re

import pyspark
from pyspark.sql import SparkSession
import pyspark.sql.functions as f
from pyspark.sql.functions import udf, current_timestamp, date_format, year, month, dayofmonth, col, coalesce, md5
from pyspark.sql.types import StringType, FloatType, StructType, StructField, BooleanType, IntegerType, ArrayType, DoubleType, MapType
from pyspark.ml.feature import Tokenizer, HashingTF, IDF, MinHashLSH
from pyspark.ml.linalg import Vectors, VectorUDT
import underthesea
import numpy as np

# CASTING FUNCTIONS
def cast_to_string(value):
    try:
        return str(value)
    except (ValueError, TypeError):
        return None

def cast_to_boolean(value):
    try:
        return bool(value)
    except (ValueError, TypeError):
        return None

def cast_to_integer(value):
    try:
        return int(value)
    except (ValueError, TypeError):
        return None
    
def cast_to_float(value):
    try:
        return float(value)
    except (ValueError, TypeError):
        return None
    
# DUPLICATION PROCESSING FUNCTIONS
@udf(returnType=VectorUDT())
def append_non_zero_to_vector(vector, append_value=0.1):
    new_vector_dim = len(vector) + 1
    new_vector_indices = list(vector.indices) + [len(vector)]
    new_vector_values = list(vector.values) + [append_value]
    new_vector = Vectors.sparse(new_vector_dim,
                                new_vector_indices,
                                new_vector_values)
    return new_vector

def get_text_tfidf_vectors(df):
    df = df.withColumn("text", f.concat_ws(" ", f.col("title"), f.col("description"), f.col("address.full_address")))

    # Calculate TF-IDF vectors
    tokenizer = Tokenizer(inputCol="text", outputCol="tokens")
    df = tokenizer.transform(df)
    hashingTF = HashingTF(inputCol="tokens", outputCol="tf")
    df = hashingTF.transform(df)
    idf = IDF(inputCol="tf", outputCol="tfidf")
    idf_model = idf.fit(df)
    df = idf_model.transform(df)
    # Append non-zero value to vectors
    df = df.withColumn("text_vector", append_non_zero_to_vector(f.col("tfidf"), f.lit(0.1)))
    return df.drop("text", "tokens", "tf", "tfidf")

def get_duplicate_df_with_minhash(df, threshhold=0.5, num_hash_tables=3, dist_col="distCol"):
    # df must already have "id" column
    minhashLSH = MinHashLSH(inputCol="text_vector", outputCol="hashes", numHashTables=num_hash_tables)
    model = minhashLSH.fit(df)
    duplicate_df = model.approxSimilarityJoin(df.select("id", "text_vector"), df.select("id", "text_vector"), 0.8, distCol=dist_col) \
                         .filter("datasetA.id < datasetB.id")  # Avoid comparing a row to itself
    duplicate_df = duplicate_df.withColumn("id", f.col("datasetA.id")) \
                               .withColumn("duplicate_with_id", f.col("datasetB.id")) \
                               .select("id", "duplicate_with_id", dist_col)
    return duplicate_df

def remove_duplicated_rows(df, remove_ids):
    # df must already have "id" column
    remove_ids = remove_ids.select("id")
    result_df = df.join(remove_ids, df["id"] == remove_ids["id"], "leftanti")
    return result_df

# TEXT PROCESSING FUNCTIONS
def get_special_chars(df: pyspark.sql.dataframe.DataFrame):
    # get concatenated text
    concatenated_text = df.select(f.concat_ws(' ', f.col('title'), f.col('description')).alias('concatenated_text'))
    all_characters = concatenated_text.rdd.flatMap(lambda x: x[0])
    special_characters = all_characters.filter(lambda c: not c.isalnum() and not c.isspace() and not c in string.punctuation)
    return set(special_characters.collect())

def get_estate_types(df: pyspark.sql.dataframe.DataFrame):
    df = df.filter(df['estate_type'].isNotNull())
    all_estate_types = df.select('estate_type').rdd.map(lambda x: x[0])
    estate_types_set = set(all_estate_types.collect())
    return estate_types_set

@udf(returnType=StringType())
def remove_special_chars(input_string, special_chars_list, at_once=False):
    if not input_string:
        return None
    if at_once:
        special_chars_string = ''.join(special_chars_list)
        translator = str.maketrans('', '', special_chars_string)
        result = input_string.translate(translator)
    else:
        result = input_string
        for c in special_chars_list:
            result = result.replace(c, '')
    return result

@udf(returnType=StringType())
def remove_duplicate_punctuation_sequence(input_string):
    def remove_duplicate_sequence(text, target_char, max_length):
        pattern_1 = re.escape(target_char) + '{' + str(max_length) + ',}'
        pattern_2 = '(' + '\s' + re.escape(target_char) + ')' + '{' + str(max_length) + ',}'
        result = re.sub(pattern_2, target_char, re.sub(pattern_1, target_char, text))
        return result
    
    if not input_string:
        return None
    result = input_string
    for punc in string.punctuation:
        if punc == '\\':
            continue
        max_length = 3 if punc == '.' else 1
        result = remove_duplicate_sequence(result, punc, max_length)
    return result

@udf(returnType=StringType())
def normalize_estate_type(input_estate_type):
    if not input_estate_type:
        return None
    estate_type_prefix = ['Cho thuá»ƒ', 'Mua bÃ¡n', 'CÄƒn há»™']
    estate_type_map = {
        'Biá»‡t thá»±, liá»n ká»': 'Biá»‡t thá»± liá»n ká»',
        'NhÃ  biá»‡t thá»± liá»n ká»': 'Biá»‡t thá»± liá»n ká»',
        'NhÃ  máº·t phá»‘': 'NhÃ  máº·t tiá»n',
        "Chung cÆ°": "Chung cÆ°",
        "CÄƒn há»™": "Chung cÆ°"
    }
    result = input_estate_type
    for prefix in estate_type_prefix:
        result = result.replace(prefix, '').strip().capitalize()
    for estate_type in estate_type_map.keys():
        if result == estate_type:
            result = estate_type_map[estate_type]
    return result

# NUMBERS PROCESSING FUNCTION
def get_lower_upper_bound(df, col_name, lower_percent=5, upper_percent=95, outlier_threshold=5):
    lower_percentile, upper_percentile = df.approxQuantile(col_name, [lower_percent/100, upper_percent/100], 0.01)
    quantile_range = upper_percentile - lower_percentile
    lower_bound = np.max([0, lower_percentile - outlier_threshold * quantile_range])
    upper_bound = upper_percentile + outlier_threshold * quantile_range
    return lower_bound, upper_bound

def get_detail_lower_upper_bound(df, col_name, lower_percent=5, upper_percent=95, outlier_threshold=5):
    quantiles_by_estate_type = (
        df.groupBy("estate_type")
        .agg(f.percentile_approx(col_name, [lower_percent/100, upper_percent/100], 100).alias("percentile_approx"))
    )
    quantiles_by_estate_type = quantiles_by_estate_type.withColumn("lower_percentile", f.col("percentile_approx").getItem(0)) \
                                                       .withColumn("upper_percentile", f.col("percentile_approx").getItem(1)) \
                                                       .withColumn("quantile_range", f.col("upper_percentile") - f.col("lower_percentile"))
    quantiles_by_estate_type = quantiles_by_estate_type.withColumn("lower_bound", f.greatest(f.col("lower_percentile") - outlier_threshold * f.col("quantile_range"), f.lit(0))) \
                                                       .withColumn("upper_bound", f.col("upper_percentile") + outlier_threshold * f.col("quantile_range"))
    
    return quantiles_by_estate_type.select("estate_type", "lower_bound", "upper_bound")

def filter_with_detail_bound(df, bound_df, join_col_name, filter_col_name):
    join_df = df.join(bound_df.alias("bound_df"), join_col_name, "inner")
    filtered_df = join_df.filter((join_df[filter_col_name] >= join_df["lower_bound"]) \
                                 & (join_df[filter_col_name] <= join_df["upper_bound"]))
    return filtered_df.drop("lower_bound", "upper_bound")

@udf(returnType=FloatType())
def price_normalize(price, square):
    if price is None:
        return None
    if isinstance(price, int) or isinstance(price, float):
        return price
    elif isinstance(price, str):
        if cast_to_float(price) is not None:
            return cast_to_float(price)
        if square is not None:
            price = underthesea.text_normalize(price)
            # CÃ¡c trÆ°á»ng há»£p thá»±c sá»± Ä‘iá»n giÃ¡ / m2
            if 'triá»‡u/ m' in price or 'triá»‡u / m' in price:
                price = float(price.split()[0]) * 1e6 * square
            # CÃ¡c trÆ°á»ng há»£p Ä‘iá»n nháº§m giÃ¡ sang giÃ¡ / m2
            elif 'tá»·/ m' in price or 'tá»· / m' in price:
                price = float(price.split()[0]) * 1e9
            else:
                price = None
        elif square is None:
            price = None
    return price

# EXTRA INFOS PROCESSING FUNCTIONS
def get_extra_info_labels(df):
    extra_infos_df = df.select("extra_infos")
    extra_infos_labels = extra_infos_df.rdd.flatMap(lambda x: list(x[0].asDict().keys())).collect()
    return set(extra_infos_labels)
    
def normalize_text_field_in_dict(dict_obj):
    result_dict = dict_obj
    for key in result_dict.keys():
        if isinstance(result_dict[key], str):
            result_dict[key] = result_dict[key].replace(',', '.')
            new_val = ''
            for c in result_dict[key]:
                if c.isalpha() or c.isnumeric() or c == '.' or c == ' ':
                    new_val += c
            result_dict[key] = new_val
    return result_dict

@udf(returnType=StructType([
    StructField('no_bedrooms', IntegerType()),
    StructField('no_bathrooms', IntegerType()),
    StructField('front_road', FloatType()),
    StructField('front_face', FloatType()),
    StructField('no_floors', IntegerType()),
    StructField('direction', StringType()),
    StructField('ultilization_square', FloatType()),
    StructField('yo_construction', IntegerType()),
]))
def normalize_extra_infos_dict(input_extra_infos_row, old_keys, new_keys, remove_keys):
    if input_extra_infos_row is None:
        return None
    old_keys = list(old_keys)
    new_keys = list(new_keys)
    remove_keys = list(remove_keys)

    # Normalize dict keys
    extra_infos_dict = input_extra_infos_row  # input_extra_infos_row Ä‘Ã£ lÃ  má»™t dict tá»« MapType
    dict_normalized_keys = {k: None for k in new_keys}

    # Define mapping of possible old keys to new keys
    key_mapping = {
        'no_bedrooms': ['Sá»‘ PhÃ²ng Ngá»§', 'Sá»‘ phÃ²ng ngá»§ :'],
        'no_bathrooms': ['Sá»‘ PhÃ²ng Táº¯m', 'Sá»‘ toilet :'],
        'front_road': ['ÄÆ°á»ng TrÆ°á»›c NhÃ '],
        'front_face': ['Máº·t Tiá»n'],
        'no_floors': ['Sá»‘ Táº§ng', 'Táº§ng :'],
        'direction': ['HÆ°á»›ng NhÃ '],
        'ultilization_square': ['Diá»‡n TÃ­ch Sá»­ Dá»¥ng'],
        'yo_construction': ['NÄƒm xÃ¢y dá»±ng']
    }

    # Assign values from possible old keys to new keys
    for new_key, possible_old_keys in key_mapping.items():
        for old_key in possible_old_keys:
            if old_key in extra_infos_dict.keys() and dict_normalized_keys[new_key] is None:
                dict_normalized_keys[new_key] = extra_infos_dict[old_key]
                break  # Stop after finding the first match

    # Remove unwanted keys
    for key in remove_keys:
        if key in dict_normalized_keys.keys():
            dict_normalized_keys.pop(key)

    # Normalize dict values
    result_dict = normalize_text_field_in_dict(dict_normalized_keys)
    
    # Type casting with English field names
    result_dict['no_bedrooms'] = cast_to_integer(result_dict['no_bedrooms']) if result_dict['no_bedrooms'] is not None else None
    result_dict['no_bathrooms'] = cast_to_integer(result_dict['no_bathrooms']) if result_dict['no_bathrooms'] is not None else None
    result_dict['front_road'] = cast_to_float(result_dict['front_road'].replace('mm', '').replace('m', '')) if result_dict['front_road'] is not None else None
    result_dict['front_face'] = cast_to_float(result_dict['front_face'].replace('mm', '').replace('m', '')) if result_dict['front_face'] is not None else None
    result_dict['no_floors'] = cast_to_integer(result_dict['no_floors']) if result_dict['no_floors'] is not None else None
    result_dict['direction'] = cast_to_string(result_dict['direction']) if result_dict['direction'] is not None else None
    result_dict['ultilization_square'] = cast_to_float(result_dict['ultilization_square'].replace('m2', '')) if result_dict['ultilization_square'] is not None else None
    result_dict['yo_construction'] = cast_to_integer(result_dict['yo_construction']) if result_dict['yo_construction'] is not None else None
    
    return result_dict

#####################################################################
# MAIN SPARK JOB
####################################################################
minio_config = {
    "spark.hadoop.fs.s3a.endpoint": os.getenv("MINIO_HOST", "http://minio-service.minio.svc.cluster.local:9000"),
    "spark.hadoop.fs.s3a.access.key": os.getenv("MINIO_USER", "bigdata123"),
    "spark.hadoop.fs.s3a.secret.key": os.getenv("MINIO_PASSWORD", "bigdata123"),
    "bucket": os.getenv("MINIO_BUCKET", "bds"),
    "spark.hadoop.fs.s3a.path.style.access": "true",
    "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem",
    "spark.hadoop.fs.s3a.connection.ssl.enabled": "false"
}
spark = SparkSession.builder \
    .appName("StreamProcessing") \
    .config("spark.driver.extraJavaOptions", "-Djava.security.properties=") \
    .config("spark.executor.extraJavaOptions", "-Djava.security.properties=") \
    .config("spark.hadoop.fs.s3a.endpoint", minio_config["spark.hadoop.fs.s3a.endpoint"]) \
    .config("spark.hadoop.fs.s3a.access.key", minio_config["spark.hadoop.fs.s3a.access.key"]) \
    .config("spark.hadoop.fs.s3a.secret.key", minio_config["spark.hadoop.fs.s3a.secret.key"]) \
    .config("spark.hadoop.fs.s3a.path.style.access", minio_config["spark.hadoop.fs.s3a.path.style.access"]) \
    .config("spark.hadoop.fs.s3a.impl", minio_config["spark.hadoop.fs.s3a.impl"]) \
    .config("spark.hadoop.fs.s3a.connection.ssl.enabled", minio_config["spark.hadoop.fs.s3a.connection.ssl.enabled"])\
    .config("spark.executor.memory", "2g") \
    .config("spark.executor.cores", "1") \
    .config("spark.streaming.kafka.maxRatePerPartition", "100") \
    .getOrCreate()
    
# [1] ---- Read from Kafka
kafka_bootstrap = os.getenv("KAFKA_BOOTSTRAP_SERVER", "my-cluster-kafka-bootstrap.kafka.svc.cluster.local:9092")
# Set up data stream
kafka_config = {
    "kafka.bootstrap.servers": kafka_bootstrap,
    "subscribe": "nhamatpho,bietthu,chungcu,nharieng",  # Subscribe 4 topic
    "startingOffsets": "earliest"
}
streaming_df = spark \
  .readStream \
  .format("kafka") \
  .option("kafka.bootstrap.servers", kafka_config["kafka.bootstrap.servers"]) \
  .option("subscribe", kafka_config["subscribe"]) \
  .option("startingOffsets", kafka_config["startingOffsets"]) \
  .load()

# [2] ----- Parse Data
schema = StructType([
    StructField("address", StructType([
        StructField("district", StringType(), nullable=True),
        StructField("full_address", StringType(), nullable=True),
        StructField("province", StringType(), nullable=True),
        StructField("ward", StringType(), nullable=True),
    ]), nullable=True),
    StructField("contact_info", StructType([
        StructField("name", StringType(), nullable=True),
        StructField("phone", ArrayType(StringType()), nullable=True),
    ]), nullable=True),
    StructField("description", StringType(), nullable=True),
    StructField("estate_type", StringType(), nullable=True),
    StructField("extra_infos", MapType(StringType(), StringType()), nullable=True),
    StructField("link", StringType(), nullable=True),
    StructField("post_date", StringType(), nullable=True),
    StructField("post_id", StringType(), nullable=True),
    StructField("price", StringType(), nullable=True),
    StructField("square", DoubleType(), nullable=True),
    StructField("title", StringType(), nullable=True),
])

json_string_df = streaming_df.selectExpr("CAST(value AS STRING) as json_string", "topic")
# Use the predefined schema from collected data
final_df = streaming_df.select(
    f.from_json(f.col("value").cast("string"), schema).alias("data"), 
    "topic", 
    f.col("timestamp").alias("kafka_timestamp")
).select("data.*", "topic", "kafka_timestamp")
# Add a id column
final_df = final_df.withColumn("id", coalesce(col("post_id").cast("string"), md5(col("link"))))

# [3] ----- Store in MinIO

# Táº¡o DataFrame riÃªng cho Raw, thÃªm partition time Ä‘á»ƒ sau nÃ y dá»… query
raw_df = final_df \
    .withColumn("year", year(current_timestamp())) \
    .withColumn("month", month(current_timestamp())) \
    .withColumn("day", dayofmonth(current_timestamp()))

print(f"[WRITE] Writing to MinIO ...")
# try:
raw_querry = raw_df.writeStream \
        .format("parquet") \
        .option("path", "s3a://{}/data/".format(minio_config["bucket"])) \
        .option("checkpointLocation", "s3a://{}/checkpoints/".format(minio_config["bucket"])) \
        .outputMode("append") \
        .partitionBy("topic", "year", "month", "day") \
        .start()

print("[DONE] Successfully write data into MinIO in bucket: {}".format(minio_config["bucket"]))
# except Exception as e:
#     print("[FAIL] Error when writing into MinIO: {}".format(e))
    

# [4] ----- Streaming processing
# Text processing
special_chars_list = ['â†’', '\u202a', '\uf0d8', 'âœ¤', '\u200c', 'Û£', 'ðŸ…–', 'â€“', 'â‚‹', 'â—', 'Â¬', 'Ì¶', 'â–¬', 'â‰ˆ', 'ðŸ«µ', 'â—‡', 'â–·', 'ðŸª·', 'â—Š', 'â€', 'ðŸ«´', '\uf05b', 'â¦', 'ï¸', 'ãŽ¡', 'ðŸ«°', 'â€²', 'âœ¥', 'âœ§', 'â™¤', 'ðŸ«¶', 'Ûœ', 'âƒ', 'Ì€', 'Ö', '\u2060', '\u206e', 'â€˜', 'âˆ', 'ðŸ…£', 'ðŸ…˜', 'â„…', '\ufeff', 'â€³', '\u200b', 'â™š', 'Ì£', 'â‚«', '\uf06e', 'âœ©', 'ðŸ…¨', 'â€™', '\xad', 'â˜…', 'Â±', '\U0001fae8', 'ï¸Ž', '\uf0f0', 'âˆ™', 'â™›', 'Ì‰', 'Ì›', 'â†', 'âœœ', 'Ã·', 'â™œ', 'Â·', 'â–', 'ã€‘', 'â', 'ðŸ«±', 'ãƒ»', 'â‚¬', 'â˜›', 'â€œ', 'â– ', '\uf046', 'ï¿¼', 'ï¿½', '\u200d', 'ðŸ« ', '\uf0e8', 'âƒ', 'â‰¥', 'ï½ž', 'âž£', 'Ì', 'ðŸª©', 'Ìƒ', '\uf02b', 'áª¥', 'ðŸªº', 'â™§', 'â‚', 'ã€‚', 'â™¡', 'ï¼Œ', 'ðŸª¸', 'ï¼š', 'Â¥', 'â', 'Ì‚', '\U0001fa77', '\uf0a7', 'à§£', 'âš˜', 'âž¢', 'â‡”', 'ã€', 'ï¼', 'âœ†', 'ðŸ«£', 'â›«', 'â–º', 'Ì†', 'âœŽ', 'â¯', 'ã€Š', '\uf076', 'â®', 'â€', 'Ìµ', 'ðŸ¥¹', 'â‰', 'Ì·', '\uf028', 'âœ½', 'Â«', 'â‡’', 'âž¤', '\uf0e0', '\U0001faad', 'â™™', '\uf0fc', 'ã€', 'âž¥', 'Â¤', 'ï¼†', 'ðŸ›‡', '\x7f', 'ï¼‰', 'â€”', 'â€', 'âž', 'ã€‹', 'â˜†', 'Ã—', 'âœž', 'âœ¿', 'â‰¤', 'ðŸ…', 'âˆš', 'Â°', 'âœ“', 'Â¡', 'â€¦', 'â€¢', 'Â»', 'âŠ', 'âž¦', '\u06dd', '\uf06c', 'Â¸']
final_df = final_df.withColumn("title", remove_special_chars("title", f.lit(special_chars_list)))
final_df = final_df.withColumn("description", remove_special_chars("description", f.lit(special_chars_list)))
final_df = final_df.withColumn("title", remove_duplicate_punctuation_sequence("title"))
final_df = final_df.withColumn("description", remove_duplicate_punctuation_sequence("description"))
final_df = final_df.withColumn("estate_type", normalize_estate_type("estate_type"))
print("Text processed.")

# Numbers processing
final_df = final_df.withColumn("price/square", f.col("price")/f.col("square"))
final_df = final_df.withColumn("price", price_normalize("price", "square"))
print("Numbers processed.")

# Extra infos processing
old_keys = [
    'Sá»‘ PhÃ²ng Ngá»§', 'Sá»‘ PhÃ²ng Táº¯m', 'ÄÆ°á»ng TrÆ°á»›c NhÃ ', 'Máº·t Tiá»n', 'Sá»‘ Táº§ng', 
    'HÆ°á»›ng NhÃ ', 'Diá»‡n TÃ­ch Sá»­ Dá»¥ng', 'NÄƒm xÃ¢y dá»±ng',  # Nguá»“n 1
    'Sá»‘ phÃ²ng ngá»§ :', 'Sá»‘ toilet :', 'Táº§ng :'  # Nguá»“n 2
]
new_keys = [
    'no_bedrooms', 'no_bathrooms', 'front_road', 'front_face', 'no_floors', 
    'direction', 'ultilization_square', 'yo_construction',
    'no_bedrooms', 'no_bathrooms', 'no_floors'  # Ãnh xáº¡ tÆ°Æ¡ng á»©ng
]
remove_keys = []
final_df = final_df.withColumn("extra_infos", normalize_extra_infos_dict("extra_infos", f.lit(old_keys), f.lit(new_keys), f.lit(remove_keys)))
print("Extra infos processed.")

final_df = final_df.withColumn("created_at", date_format(current_timestamp(), "yyyy/MM/dd HH:mm:ss"))
print("Added created_at field.")

# [5] -------- Output
# Output two queries, one to console and one to Elasticsearch
es_config_base = {
    "es.nodes": os.getenv("ES_HOST", "my-es-cluster-es-http.elastic.svc.cluster.local"),
    "es.port": os.getenv("ES_PORT", "9200"),
    "es.net.http.auth.user": os.getenv("ES_USER", "elastic"),
    "es.net.http.auth.pass": os.getenv("ES_PASS", "83zobLJ9694Ww5qq982qcJYt"),
    "es.nodes.wan.only": "true",
    "es.nodes.discovery": "false",
    "es.batch.write.retry.count": "3",
    "es.index.auto.create": "true",
    "es.spark.sql.streaming.sink.log.enabled": "true",
    "es.write.operation": "upsert",  # KÃ­ch hoáº¡t cháº¿ Ä‘á»™ cáº­p nháº­t
    "es.mapping.id": "id"            # NÃ³i cho Spark biáº¿t: "HÃ£y láº¥y cá»™t 'id' lÃ m khÃ³a chÃ­nh"
}

# Äá»‹nh nghÄ©a Ã¡nh xáº¡ topic vÃ  index
topics_indices = {
    "nhamatpho": "nhapho_index",
    "bietthu": "bietthu_index",
    "chungcu": "chungcu_index",
    "nharieng": "nharieng_index"
}

# Khá»Ÿi Ä‘á»™ng cÃ¡c luá»“ng ghi
queries = []

# Ghi ra console
console_query = final_df.writeStream \
    .outputMode("append") \
    .format("console") \
    .start()
queries.append(console_query)

# Ghi vÃ o Elasticsearch theo topic
for topic, index in topics_indices.items():
    topic_df = final_df.filter(f.col("topic") == topic)
    es_query = topic_df.writeStream \
        .outputMode("append") \
        .format("org.elasticsearch.spark.sql") \
        .option("es.nodes", es_config_base["es.nodes"]) \
        .option("es.port", es_config_base["es.port"]) \
        .option("es.net.http.auth.user", es_config_base["es.net.http.auth.user"]) \
        .option("es.net.http.auth.pass", es_config_base["es.net.http.auth.pass"]) \
        .option("es.resource", index) \
        .option("es.nodes.wan.only", es_config_base["es.nodes.wan.only"]) \
        .option("es.index.auto.create", es_config_base["es.index.auto.create"]) \
        .option("es.nodes.discovery", es_config_base["es.nodes.discovery"]) \
        .option("es.batch.write.retry.count", es_config_base["es.batch.write.retry.count"]) \
        .option("es.spark.sql.streaming.sink.log.enabled", es_config_base["es.spark.sql.streaming.sink.log.enabled"]) \
        .option("es.write.operation", es_config_base["es.write.operation"])\
        .option("es.mapping.id", es_config_base["es.mapping.id"])\
        .option("checkpointLocation", f"s3a://checkpoint/{topic}") \
        .start()
    queries.append(es_query)

# Chá» báº¥t ká»³ luá»“ng nÃ o hoÃ n thÃ nh
try:
    spark.streams.awaitAnyTermination()
except Exception as e:
    print(f"Error in streaming: {e}")
finally:
    spark.stop()