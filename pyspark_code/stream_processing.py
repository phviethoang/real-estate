import os
os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.elasticsearch:elasticsearch-spark-30_2.12:7.12.0,commons-httpclient:commons-httpclient:3.1,org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'
import sys
import string
import re

import pyspark
from pyspark.sql import SparkSession
import pyspark.sql.functions as f
from pyspark.sql.types import StringType, FloatType, StructType, StructField, BooleanType, IntegerType, ArrayType, DoubleType
import underthesea
import numpy as np
from process_duplication import *
from process_text import *
from process_number import *
from process_extra_infos import *


spark = SparkSession.builder.master("local[1]").getOrCreate()
# spark.sparkContext.setLogLevel("ERROR")

# Set up data stream
kafka_config = {
    "kafka.bootstrap.servers": "20.239.82.205:9192,20.239.82.205:9292,20.239.82.205:9392",
    "subscribe": "bds,i-batdongsan,nhadatviet,test",
    "startingOffsets": "latest"
}
streaming_df = spark \
  .readStream \
  .format("kafka") \
  .option("kafka.bootstrap.servers", kafka_config["kafka.bootstrap.servers"]) \
  .option("subscribe", kafka_config["subscribe"]) \
  .option("startingOffsets", kafka_config["startingOffsets"]) \
  .load()

# Predefined object from collected data
special_chars_list = ['‚Üí', '\u202a', '\uf0d8', '‚ú§', '\u200c', '€£', 'üÖñ', '‚Äì', '‚Çã', '‚óè', '¬¨', 'Ã∂', '‚ñ¨', '‚âà', 'ü´µ', '‚óá', '‚ñ∑', 'ü™∑', '‚óä', '‚Äê', 'ü´¥', '\uf05b', '‚¶Å', 'Ô∏è', '„é°', 'ü´∞', '‚Ä≤', '‚ú•', '‚úß', '‚ô§', 'ü´∂', '€ú', '‚ùÉ', 'ÃÄ', '÷ç', '\u2060', '\u206e', '‚Äò', '‚ùà', 'üÖ£', 'üÖò', '‚ÑÖ', '\ufeff', '‚Ä≥', '\u200b', '‚ôö', 'Ã£', '‚Ç´', '\uf06e', '‚ú©', 'üÖ®', '‚Äô', '\xad', '‚òÖ', '¬±', '\U0001fae8', 'Ô∏é', '\uf0f0', '‚àô', '‚ôõ', 'Ãâ', 'Ãõ', '‚ùÜ', '‚úú', '√∑', '‚ôú', '¬∑', '‚ùñ', '„Äë', '‚ùÅ', 'ü´±', '„Éª', '‚Ç¨', '‚òõ', '‚Äú', '‚ñ†', '\uf046', 'Ôøº', 'ÔøΩ', '\u200d', 'ü´†', '\uf0e8', '‚ÅÉ', '‚â•', 'ÔΩû', '‚û£', 'ÃÅ', 'ü™©', 'ÃÉ', '\uf02b', '·™•', 'ü™∫', '‚ôß', '‚ùÇ', '„ÄÇ', '‚ô°', 'Ôºå', 'ü™∏', 'Ôºö', '¬•', '‚ùù', 'ÃÇ', '\U0001fa77', '\uf0a7', '‡ß£', '‚öò', '‚û¢', '‚áî', '„ÄÅ', 'Ôºç', '‚úÜ', 'ü´£', '‚õ´', '‚ñ∫', 'ÃÜ', '‚úé', '‚ùØ', '„Ää', '\uf076', '‚ùÆ', '‚ùÄ', 'Ãµ', 'ü•π', '‚ùâ', 'Ã∑', '\uf028', '‚úΩ', '¬´', '‚áí', '‚û§', '\uf0e0', '\U0001faad', '‚ôô', '\uf0fc', '„Äê', '‚û•', '¬§', 'ÔºÜ', 'üõá', '\x7f', 'Ôºâ', '‚Äî', '‚Äù', '‚ùû', '„Äã', '‚òÜ', '√ó', '‚úû', '‚úø', '‚â§', 'üÖê', '‚àö', '¬∞', '‚úì', '¬°', '‚Ä¶', '‚Ä¢', '¬ª', '‚ùä', '‚û¶', '\u06dd', '\uf06c', '¬∏']
old_keys = ['Chi·ªÅu d√†i', 'Chi·ªÅu ngang', 'Ch√≠nh ch·ªß', 'Ch·ªï ƒë·ªÉ xe h∆°i', 'H∆∞·ªõng', 'Lo·∫°i tin', 'L·ªô gi·ªõi', 'Nh√† b·∫øp', 'Ph√°p l√Ω', 'Ph√≤ng ƒÉn', 'S√¢n th∆∞·ª£ng', 'S·ªë l·∫ßu', 'S·ªë ph√≤ng ng·ªß', 'S·ªë ph√≤ng ng·ªß :', 'S·ªë toilet :', 'T·∫ßng :']
new_keys = ['Chi·ªÅu d√†i', 'Chi·ªÅu ngang', 'Ch√≠nh ch·ªß', 'Ch·ªó ƒë·ªÉ xe h∆°i', 'H∆∞·ªõng', 'remove', 'L·ªô gi·ªõi', 'Nh√† b·∫øp', 'Ph√°p l√Ω', 'Ph√≤ng ƒÉn', 'S√¢n th∆∞·ª£ng', 'S·ªë l·∫ßu', 'S·ªë ph√≤ng ng·ªß', 'S·ªë ph√≤ng ng·ªß', 'S·ªë toilet', 'T·∫ßng']
remove_keys = ['remove']

schema = StructType([
    StructField("address", StructType([
        StructField("district", StringType(), nullable=True),
        StructField("full_address", StringType(), nullable=True),
        StructField("province", StringType(), nullable=True),
        StructField("ward", StringType(), nullable=True),
    ]), nullable=True),
    StructField("contact_info", StructType([
        StructField("name", StringType(), nullable=True),
        StructField("phone", ArrayType(StringType()), nullable=True),
    ]), nullable=True),
    StructField("description", StringType(), nullable=True),
    StructField("estate_type", StringType(), nullable=True),
    StructField("extra_infos", StructType([
        StructField("Chi·ªÅu d√†i", StringType(), nullable=True),
        StructField("Chi·ªÅu ngang", StringType(), nullable=True),
        StructField("Ch√≠nh ch·ªß", BooleanType(), nullable=True),
        StructField("Ch·ªï ƒë·ªÉ xe h∆°i", BooleanType(), nullable=True),
        StructField("H∆∞·ªõng", StringType(), nullable=True),
        StructField("Lo·∫°i tin", StringType(), nullable=True),
        StructField("L·ªô gi·ªõi", StringType(), nullable=True),
        StructField("Nh√† b·∫øp", BooleanType(), nullable=True),
        StructField("Ph√°p l√Ω", StringType(), nullable=True),
        StructField("Ph√≤ng ƒÉn", BooleanType(), nullable=True),
        StructField("S√¢n th∆∞·ª£ng", BooleanType(), nullable=True),
        StructField("S·ªë l·∫ßu", StringType(), nullable=True),
        StructField("S·ªë ph√≤ng ng·ªß", StringType(), nullable=True),
        StructField("S·ªë ph√≤ng ng·ªß :: string", StringType(), nullable=True),
        StructField("S·ªë toilet :: string", StringType(), nullable=True),
        StructField("T·∫ßng :: string", StringType(), nullable=True),
    ]), nullable=True),
    StructField("link", StringType(), nullable=True),
    StructField("post_date", StringType(), nullable=True),
    StructField("post_id", StringType(), nullable=True),
    StructField("price", StringType(), nullable=True),
    StructField("square", DoubleType(), nullable=True),
    StructField("title", StringType(), nullable=True),
])

json_string_df = streaming_df.selectExpr("CAST(value AS STRING) as json_string")
# Use the predefined schema from colected data
parsed_json_df = json_string_df.select(
    f.from_json("json_string", schema).alias("data")
)
final_df = parsed_json_df.select("data.*")

# Text processing
def remove_special_chars(input_string, special_chars_list, at_once=False):
    if not input_string:
        return None
    if at_once:
        special_chars_string = ''.join()
        translator = str.maketrans('', '', special_chars_string)
        result = input_string.translate(translator)
    else:
        result = input_string
        for c in special_chars_list:
            result = result.replace(c, '')
    return result

def remove_special_chars_uds(special_chars_list):
    return udf(lambda s: remove_special_chars(s, special_chars_list), returnType=StringType())

final_df = final_df.withColumn("title", remove_special_chars_uds(special_chars_list)(f.col("title")))
final_df = final_df.withColumn("description", remove_special_chars_uds(special_chars_list)(f.col("description")))
final_df = final_df.withColumn("title", remove_duplicate_punctuation_sequence("title"))
final_df = final_df.withColumn("description", remove_duplicate_punctuation_sequence("description"))
final_df = final_df.withColumn("estate_type", normalize_estate_type("estate_type"))
print("Text processed.")

# Numbers processing
final_df = final_df.withColumn("price/square", f.col("price")/f.col("square"))
final_df = final_df.withColumn("price", price_normalize("price", "square"))
print("Numbers processed.")

# Extra infos processing
def normalize_extra_infos_dict(input_extra_infos_row, old_keys, new_keys, remove_keys):
    if input_extra_infos_row is None:
        return None
    old_keys = list(old_keys)
    new_keys = list(new_keys)
    remove_keys = list(remove_keys)
    assert len(old_keys) == len(new_keys)

    # Normalize dict keys
    extra_infos_dict = input_extra_infos_row.asDict()
    dict_nomalized_keys = {k: None for k in new_keys}

    for old_key, new_key in zip(old_keys, new_keys):
        if old_key in extra_infos_dict.keys():
            if new_key in dict_nomalized_keys.keys() and dict_nomalized_keys[new_key] is None \
                or new_key not in dict_nomalized_keys.keys():
                dict_nomalized_keys[new_key] = extra_infos_dict[old_key]
        else:
            dict_nomalized_keys[new_key] = None
    for key in remove_keys:
        if key in dict_nomalized_keys.keys():
            dict_nomalized_keys.pop(key)
    # Normalize dict values
    result_dict = normalize_text_field_in_dict(dict_nomalized_keys)
    result_dict['Chi·ªÅu d√†i'] = cast_to_float(dict_nomalized_keys['Chi·ªÅu d√†i'].replace('m', '')) if result_dict['Chi·ªÅu d√†i'] is not None else None
    result_dict['Chi·ªÅu ngang'] = cast_to_float(dict_nomalized_keys['Chi·ªÅu ngang'].replace('m', '')) if result_dict['Chi·ªÅu ngang'] is not None else None
    result_dict['Ch√≠nh ch·ªß'] = cast_to_boolean(dict_nomalized_keys['Ch√≠nh ch·ªß']) if result_dict['Ch√≠nh ch·ªß'] is not None else None
    result_dict['Ch·ªó ƒë·ªÉ xe h∆°i'] = cast_to_boolean(dict_nomalized_keys['Ch·ªó ƒë·ªÉ xe h∆°i']) if result_dict['Ch·ªó ƒë·ªÉ xe h∆°i'] is not None else None
    result_dict['H∆∞·ªõng'] = cast_to_string(dict_nomalized_keys['H∆∞·ªõng']) if result_dict['H∆∞·ªõng'] is not None else None
    result_dict['L·ªô gi·ªõi'] = cast_to_float(dict_nomalized_keys['L·ªô gi·ªõi'].replace('m', '')) if result_dict['L·ªô gi·ªõi'] is not None else None
    result_dict['Nh√† b·∫øp'] = cast_to_boolean(dict_nomalized_keys['Nh√† b·∫øp']) if result_dict['Nh√† b·∫øp'] is not None else None
    result_dict['Ph√°p l√Ω'] = cast_to_string(dict_nomalized_keys['Ph√°p l√Ω']) if result_dict['Ph√°p l√Ω'] is not None else None
    result_dict['Ph√≤ng ƒÉn'] = cast_to_boolean(dict_nomalized_keys['Ph√≤ng ƒÉn']) if result_dict['Ph√≤ng ƒÉn'] is not None else None
    result_dict['S√¢n th∆∞·ª£ng'] = cast_to_boolean(dict_nomalized_keys['S√¢n th∆∞·ª£ng']) if result_dict['S√¢n th∆∞·ª£ng'] is not None else None
    result_dict['S·ªë l·∫ßu'] = cast_to_integer(dict_nomalized_keys['S·ªë l·∫ßu']) if result_dict['S·ªë l·∫ßu'] is not None else None
    result_dict['S·ªë ph√≤ng ng·ªß'] = cast_to_integer(dict_nomalized_keys['S·ªë ph√≤ng ng·ªß']) if result_dict['S·ªë ph√≤ng ng·ªß'] is not None else None
    result_dict['S·ªë toilet'] = cast_to_integer(dict_nomalized_keys['S·ªë toilet']) if result_dict['S·ªë toilet'] is not None else None
    result_dict['T·∫ßng'] = cast_to_integer(dict_nomalized_keys['T·∫ßng']) if result_dict['T·∫ßng'] is not None else None
    return result_dict

def normalize_extra_infos_dict_udf(old_keys, new_keys, remove_keys):
    return udf(lambda d: normalize_extra_infos_dict(d, old_keys, new_keys, remove_keys),returnType=StructType([
    StructField('Chi·ªÅu d√†i', FloatType()),
    StructField('Chi·ªÅu ngang', FloatType()),
    StructField('Ch√≠nh ch·ªß', BooleanType()),
    StructField('Ch·ªó ƒë·ªÉ xe h∆°i', BooleanType()),
    StructField('H∆∞·ªõng', StringType()),
    StructField('L·ªô gi·ªõi', FloatType()),
    StructField('Nh√† b·∫øp', BooleanType()),
    StructField('Ph√°p l√Ω', StringType()),
    StructField('Ph√≤ng ƒÉn', BooleanType()),
    StructField('S√¢n th∆∞·ª£ng', BooleanType()),
    StructField('S·ªë l·∫ßu', IntegerType()),
    StructField('S·ªë ph√≤ng ng·ªß', IntegerType()),
    StructField('S·ªë toilet', IntegerType()),
    StructField('T·∫ßng', IntegerType()),
]))

final_df = final_df.withColumn("extra_infos", normalize_extra_infos_dict_udf(old_keys, new_keys, remove_keys)(f.col("extra_infos")))
print("Extra infos processed.")

# Output two queries, one to console and one to elasticsearch
console_query = final_df.writeStream \
    .outputMode("append") \
    .format("console") \
    .start()

es_config = {
    "es.nodes": "20.239.82.205",
    "es.port": "9200",
    "es.resource": "haihp02_test_streaming_index",
    "es.nodes.wan.only": "true",
    "es.nodes.discovery": "false"
}
elasticsearch_query = final_df.writeStream \
    .outputMode("append") \
    .format("org.elasticsearch.spark.sql") \
    .option("es.nodes", es_config["es.nodes"]) \
    .option("es.port", es_config["es.port"]) \
    .option("es.resource", es_config["es.resource"]) \
    .option("es.nodes.wan.only", es_config["es.nodes.wan.only"]) \
    .option("es.nodes.discovery", es_config["es.nodes.discovery"]) \
    .option("checkpointLocation", "./elasticsearch_log_checkpoint") \
    .start()

console_query.awaitTermination()
elasticsearch_query.awaitTermination()

spark.stop()
